# What Is Entropy? A Measure of Just How Little We Really Know.
> Exactly 200 years ago, a French engineer introduced an idea that would quantify the universe’s inexorable slide into decay. But entropy, as it’s currently understood, is less a fact about the world than a reflection of our growing ignorance. Embracing that truth is leading to a rethink of everything from rational decision-making to the limits of machines.

Life is an anthology of destruction. Everything you build eventually breaks. Everyone you love will die. Any sense of order or stability inevitably crumbles. The entire universe follows a dismal trek toward a dull state of ultimate turmoil.

To keep track of this cosmic decay, physicists employ a concept called entropy. Entropy is a measure of disorderliness, and the declaration that entropy is always on the rise — known as the second law of thermodynamics — is among nature’s most inescapable commandments.

I have long felt haunted by the universal tendency toward messiness. Order is fragile. It takes months of careful planning and artistry to craft a vase but an instant to demolish it with a soccer ball. We spend our lives struggling to make sense of a chaotic and unpredictable world, where any attempt to establish control seems only to backfire. The second law demands that machines can never be perfectly efficient, which implies that whenever structure arises in the universe, it ultimately serves only to dissipate energy further — be it a star that eventually explodes or a living organism converting food into heat. We are, despite our best intentions, agents of entropy.

“Nothing in life is certain except death, taxes and the second law of thermodynamics,” wrote(opens a new tab) Seth Lloyd, a physicist at the Massachusetts Institute of Technology. There’s no sidestepping this directive. The growth of entropy is deeply entwined with our most basic experiences, accounting for why time runs forward and why the world appears deterministic rather than quantum mechanically uncertain.

But despite its fundamental importance, entropy is perhaps the most divisive concept in physics. “Entropy has always been a problem,” Lloyd told me. The confusion stems in part from the way the term gets tossed and twisted between disciplines — it has similar but distinct meanings in everything from physics to information theory to ecology. But it’s also because truly wrapping one’s head around entropy requires taking some deeply uncomfortable philosophical leaps.

As physicists have worked to unite seemingly disparate fields over the past century, they have cast entropy in a new light — turning the microscope back on the seer and shifting the notion of disorder to one of ignorance. Entropy is seen not as a property intrinsic to a system but as one that’s relative to an observer who interacts with that system. This modern view illuminates the deep link between information and energy, which is now helping to usher in a mini-industrial revolution on the smallest of scales.

Two hundred years after the seeds of entropy were first sown, what’s emerging is a conception of this quantity that’s more opportunistic than nihilistic. The conceptual evolution is upending the old way of thinking, not just about entropy, but about the purpose of science and our role in the universe.

## The Motive Power of Fire

The notion of entropy grew out of an attempt at perfecting machinery during the industrial revolution. A 28-year-old French military engineer named Sadi Carnot set out to calculate the ultimate efficiency of the steam-powered engine. In 1824, he published a 118-page book(opens a new tab) titled Reflections on the Motive Power of Fire, which he sold on the banks of the Seine for 3 francs. Carnot’s book was largely disregarded by the scientific community, and he died several years later of cholera. His body was burned, as were many of his papers. But some copies of his book survived, and in them lay the embers of a new science of thermodynamics — the motive power of fire.

Carnot realized that the steam engine is, at its core, a machine that exploits the tendency for heat to flow from hot objects to cold ones. He drew up the most efficient engine conceivable, instituting a bound on the fraction of heat that can be converted to work, a result now known as Carnot’s theorem. His most consequential statement comes as a caveat on the last page of the book: “We should not expect ever to utilize in practice all the motive power of combustibles.” Some energy will always be dissipated through friction, vibration, or another unwanted form of motion. Perfection is unattainable.